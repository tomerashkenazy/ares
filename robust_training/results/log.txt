[2025-08-23 18:19:03 ARES 2.0] WARNING: Neither APEX or native Torch AMP is available, using float32. Install NVIDA apex or upgrade to PyTorch 1.6
[2025-08-23 18:19:03 ARES 2.0] INFO: Creating model: resnet50
[2025-08-23 18:19:04 ARES 2.0] INFO: Model resnet50 created, param count:25557032
[2025-08-23 18:19:04 ARES 2.0] INFO: AMP not enabled. Training in float32.
[2025-08-23 18:19:04 ARES 2.0] INFO: Using native Torch DistributedDataParallel.
[2025-08-23 18:19:08 ARES 2.0] INFO: Scheduled epochs: 1
[2025-08-23 18:19:08 ARES 2.0] INFO: Start training for 1 epochs
[2025-08-23 18:19:38 ARES 2.0] INFO: Train: [0/1] [   0/10009 (  0%)]  Loss: 6.923 (6.92)  Time: 30.022s,    4.26/s  (30.022s,    4.26/s)  LR: 1.000e-04  Data: 4.457 (4.457)
[2025-08-23 18:21:41 ARES 2.0] INFO: Train: [0/1] [  50/10009 (  0%)]  Loss: 6.922 (6.92)  Time: 2.483s,   51.56/s  (2.994s,   42.75/s)  LR: 1.499e-04  Data: 0.001 (0.088)
[2025-08-23 18:23:45 ARES 2.0] INFO: Train: [0/1] [ 100/10009 (  1%)]  Loss: 6.924 (6.92)  Time: 2.482s,   51.57/s  (2.741s,   46.70/s)  LR: 1.997e-04  Data: 0.001 (0.045)
[2025-08-23 18:25:49 ARES 2.0] INFO: Train: [0/1] [ 150/10009 (  1%)]  Loss: 6.917 (6.92)  Time: 2.497s,   51.27/s  (2.656s,   48.19/s)  LR: 2.496e-04  Data: 0.001 (0.030)
[2025-08-23 18:27:53 ARES 2.0] INFO: Train: [0/1] [ 200/10009 (  2%)]  Loss: 6.952 (6.93)  Time: 2.482s,   51.56/s  (2.613s,   48.99/s)  LR: 2.994e-04  Data: 0.001 (0.023)
[2025-08-23 18:29:57 ARES 2.0] INFO: Train: [0/1] [ 250/10009 (  2%)]  Loss: 6.934 (6.93)  Time: 2.481s,   51.60/s  (2.587s,   49.48/s)  LR: 3.493e-04  Data: 0.001 (0.019)
[2025-08-23 18:32:02 ARES 2.0] INFO: Train: [0/1] [ 300/10009 (  3%)]  Loss: 6.931 (6.93)  Time: 2.484s,   51.53/s  (2.570s,   49.80/s)  LR: 3.991e-04  Data: 0.001 (0.016)
[2025-08-23 18:34:06 ARES 2.0] INFO: Train: [0/1] [ 350/10009 (  3%)]  Loss: 6.934 (6.93)  Time: 2.489s,   51.42/s  (2.558s,   50.04/s)  LR: 4.490e-04  Data: 0.001 (0.014)
[2025-08-23 18:36:10 ARES 2.0] INFO: Train: [0/1] [ 400/10009 (  4%)]  Loss: 6.904 (6.93)  Time: 2.481s,   51.59/s  (2.549s,   50.23/s)  LR: 4.988e-04  Data: 0.001 (0.012)
[2025-08-23 18:38:14 ARES 2.0] INFO: Train: [0/1] [ 450/10009 (  4%)]  Loss: 6.896 (6.92)  Time: 2.483s,   51.54/s  (2.541s,   50.37/s)  LR: 5.487e-04  Data: 0.001 (0.011)
[2025-08-23 18:40:18 ARES 2.0] INFO: Train: [0/1] [ 500/10009 (  5%)]  Loss: 6.896 (6.92)  Time: 2.483s,   51.55/s  (2.535s,   50.49/s)  LR: 5.986e-04  Data: 0.001 (0.010)
[2025-08-23 18:42:23 ARES 2.0] INFO: Train: [0/1] [ 550/10009 (  5%)]  Loss: 6.898 (6.92)  Time: 2.485s,   51.51/s  (2.531s,   50.58/s)  LR: 6.484e-04  Data: 0.001 (0.009)
[2025-08-23 18:44:27 ARES 2.0] INFO: Train: [0/1] [ 600/10009 (  6%)]  Loss: 6.916 (6.92)  Time: 2.483s,   51.55/s  (2.527s,   50.66/s)  LR: 6.983e-04  Data: 0.001 (0.008)
[2025-08-23 18:46:31 ARES 2.0] INFO: Train: [0/1] [ 650/10009 (  6%)]  Loss: 6.919 (6.92)  Time: 2.484s,   51.53/s  (2.523s,   50.73/s)  LR: 7.481e-04  Data: 0.001 (0.008)
[2025-08-23 18:48:35 ARES 2.0] INFO: Train: [0/1] [ 700/10009 (  7%)]  Loss: 6.904 (6.92)  Time: 2.483s,   51.56/s  (2.520s,   50.78/s)  LR: 7.980e-04  Data: 0.001 (0.007)
[2025-08-23 18:50:39 ARES 2.0] INFO: Train: [0/1] [ 750/10009 (  7%)]  Loss: 6.913 (6.92)  Time: 2.480s,   51.61/s  (2.518s,   50.83/s)  LR: 8.478e-04  Data: 0.001 (0.007)
[2025-08-23 18:52:43 ARES 2.0] INFO: Train: [0/1] [ 800/10009 (  8%)]  Loss: 6.895 (6.92)  Time: 2.484s,   51.53/s  (2.516s,   50.88/s)  LR: 8.977e-04  Data: 0.001 (0.007)
[2025-08-23 18:54:48 ARES 2.0] INFO: Train: [0/1] [ 850/10009 (  8%)]  Loss: 6.889 (6.91)  Time: 2.483s,   51.54/s  (2.514s,   50.92/s)  LR: 9.475e-04  Data: 0.001 (0.006)
[2025-08-23 18:56:52 ARES 2.0] INFO: Train: [0/1] [ 900/10009 (  9%)]  Loss: 6.909 (6.91)  Time: 2.482s,   51.57/s  (2.512s,   50.95/s)  LR: 9.974e-04  Data: 0.001 (0.006)
[2025-08-23 18:58:56 ARES 2.0] INFO: Train: [0/1] [ 950/10009 (  9%)]  Loss: 6.874 (6.91)  Time: 2.482s,   51.57/s  (2.511s,   50.98/s)  LR: 1.047e-03  Data: 0.001 (0.006)
[2025-08-23 19:01:00 ARES 2.0] INFO: Train: [0/1] [1000/10009 ( 10%)]  Loss: 6.880 (6.91)  Time: 2.482s,   51.58/s  (2.509s,   51.01/s)  LR: 1.097e-03  Data: 0.001 (0.005)
[2025-08-23 19:03:04 ARES 2.0] INFO: Train: [0/1] [1050/10009 ( 10%)]  Loss: 6.881 (6.91)  Time: 2.482s,   51.56/s  (2.508s,   51.04/s)  LR: 1.147e-03  Data: 0.001 (0.005)
[2025-08-23 19:05:08 ARES 2.0] INFO: Train: [0/1] [1100/10009 ( 11%)]  Loss: 6.855 (6.91)  Time: 2.482s,   51.57/s  (2.507s,   51.06/s)  LR: 1.197e-03  Data: 0.001 (0.005)
[2025-08-23 19:07:12 ARES 2.0] INFO: Train: [0/1] [1150/10009 ( 11%)]  Loss: 6.879 (6.91)  Time: 2.481s,   51.59/s  (2.506s,   51.08/s)  LR: 1.247e-03  Data: 0.001 (0.005)
[2025-08-23 19:09:16 ARES 2.0] INFO: Train: [0/1] [1200/10009 ( 12%)]  Loss: 6.871 (6.90)  Time: 2.481s,   51.59/s  (2.505s,   51.10/s)  LR: 1.297e-03  Data: 0.001 (0.005)
[2025-08-23 19:11:21 ARES 2.0] INFO: Train: [0/1] [1250/10009 ( 12%)]  Loss: 6.850 (6.90)  Time: 2.482s,   51.57/s  (2.504s,   51.12/s)  LR: 1.346e-03  Data: 0.001 (0.004)
[2025-08-23 19:13:25 ARES 2.0] INFO: Train: [0/1] [1300/10009 ( 13%)]  Loss: 6.875 (6.90)  Time: 2.485s,   51.51/s  (2.503s,   51.14/s)  LR: 1.396e-03  Data: 0.001 (0.004)
[2025-08-23 19:15:29 ARES 2.0] INFO: Train: [0/1] [1350/10009 ( 13%)]  Loss: 6.847 (6.90)  Time: 2.485s,   51.52/s  (2.502s,   51.15/s)  LR: 1.446e-03  Data: 0.001 (0.004)
[2025-08-23 19:17:33 ARES 2.0] INFO: Train: [0/1] [1400/10009 ( 14%)]  Loss: 6.914 (6.90)  Time: 2.484s,   51.53/s  (2.502s,   51.17/s)  LR: 1.496e-03  Data: 0.001 (0.004)
[2025-08-23 19:19:37 ARES 2.0] INFO: Train: [0/1] [1450/10009 ( 14%)]  Loss: 6.876 (6.90)  Time: 2.484s,   51.53/s  (2.501s,   51.18/s)  LR: 1.546e-03  Data: 0.001 (0.004)
[2025-08-23 19:21:41 ARES 2.0] INFO: Train: [0/1] [1500/10009 ( 15%)]  Loss: 6.862 (6.90)  Time: 2.483s,   51.55/s  (2.500s,   51.19/s)  LR: 1.596e-03  Data: 0.001 (0.004)
[2025-08-23 19:23:46 ARES 2.0] INFO: Train: [0/1] [1550/10009 ( 15%)]  Loss: 6.848 (6.90)  Time: 2.483s,   51.56/s  (2.500s,   51.20/s)  LR: 1.646e-03  Data: 0.001 (0.004)
[2025-08-23 19:25:50 ARES 2.0] INFO: Train: [0/1] [1600/10009 ( 16%)]  Loss: 6.817 (6.89)  Time: 2.482s,   51.57/s  (2.499s,   51.21/s)  LR: 1.695e-03  Data: 0.001 (0.004)
[2025-08-23 19:27:54 ARES 2.0] INFO: Train: [0/1] [1650/10009 ( 16%)]  Loss: 6.803 (6.89)  Time: 2.485s,   51.51/s  (2.499s,   51.22/s)  LR: 1.745e-03  Data: 0.001 (0.004)
[2025-08-23 19:29:58 ARES 2.0] INFO: Train: [0/1] [1700/10009 ( 17%)]  Loss: 6.809 (6.89)  Time: 2.483s,   51.54/s  (2.498s,   51.23/s)  LR: 1.795e-03  Data: 0.001 (0.004)
[2025-08-23 19:32:02 ARES 2.0] INFO: Train: [0/1] [1750/10009 ( 17%)]  Loss: 6.796 (6.89)  Time: 2.485s,   51.51/s  (2.498s,   51.24/s)  LR: 1.845e-03  Data: 0.001 (0.003)
[2025-08-23 19:34:07 ARES 2.0] INFO: Train: [0/1] [1800/10009 ( 18%)]  Loss: 6.793 (6.88)  Time: 2.486s,   51.49/s  (2.498s,   51.25/s)  LR: 1.895e-03  Data: 0.001 (0.003)
[2025-08-23 19:36:11 ARES 2.0] INFO: Train: [0/1] [1850/10009 ( 18%)]  Loss: 6.821 (6.88)  Time: 2.485s,   51.52/s  (2.497s,   51.26/s)  LR: 1.945e-03  Data: 0.001 (0.003)
[2025-08-23 19:38:15 ARES 2.0] INFO: Train: [0/1] [1900/10009 ( 19%)]  Loss: 6.887 (6.88)  Time: 2.481s,   51.59/s  (2.497s,   51.26/s)  LR: 1.994e-03  Data: 0.001 (0.003)
[2025-08-23 19:40:19 ARES 2.0] INFO: Train: [0/1] [1950/10009 ( 19%)]  Loss: 6.821 (6.88)  Time: 2.485s,   51.50/s  (2.497s,   51.27/s)  LR: 2.044e-03  Data: 0.001 (0.003)
[2025-08-23 19:42:23 ARES 2.0] INFO: Train: [0/1] [2000/10009 ( 20%)]  Loss: 6.818 (6.88)  Time: 2.485s,   51.51/s  (2.496s,   51.28/s)  LR: 2.094e-03  Data: 0.001 (0.003)
[2025-08-23 19:44:27 ARES 2.0] INFO: Train: [0/1] [2050/10009 ( 20%)]  Loss: 6.789 (6.88)  Time: 2.482s,   51.57/s  (2.496s,   51.28/s)  LR: 2.144e-03  Data: 0.001 (0.003)
[2025-08-23 19:46:32 ARES 2.0] INFO: Train: [0/1] [2100/10009 ( 21%)]  Loss: 6.732 (6.87)  Time: 2.485s,   51.51/s  (2.496s,   51.29/s)  LR: 2.194e-03  Data: 0.001 (0.003)
[2025-08-23 19:48:36 ARES 2.0] INFO: Train: [0/1] [2150/10009 ( 21%)]  Loss: 6.756 (6.87)  Time: 2.483s,   51.56/s  (2.495s,   51.30/s)  LR: 2.244e-03  Data: 0.001 (0.003)
[2025-08-23 19:50:40 ARES 2.0] INFO: Train: [0/1] [2200/10009 ( 22%)]  Loss: 6.795 (6.87)  Time: 2.484s,   51.53/s  (2.495s,   51.30/s)  LR: 2.294e-03  Data: 0.001 (0.003)
[2025-08-23 19:52:44 ARES 2.0] INFO: Train: [0/1] [2250/10009 ( 22%)]  Loss: 6.781 (6.87)  Time: 2.485s,   51.51/s  (2.495s,   51.31/s)  LR: 2.343e-03  Data: 0.001 (0.003)
[2025-08-23 19:54:48 ARES 2.0] INFO: Train: [0/1] [2300/10009 ( 23%)]  Loss: 6.761 (6.87)  Time: 2.483s,   51.56/s  (2.495s,   51.31/s)  LR: 2.393e-03  Data: 0.001 (0.003)
[2025-08-23 19:56:53 ARES 2.0] INFO: Train: [0/1] [2350/10009 ( 23%)]  Loss: 6.831 (6.86)  Time: 2.483s,   51.55/s  (2.494s,   51.32/s)  LR: 2.443e-03  Data: 0.001 (0.003)
[2025-08-23 19:58:57 ARES 2.0] INFO: Train: [0/1] [2400/10009 ( 24%)]  Loss: 6.817 (6.86)  Time: 2.483s,   51.54/s  (2.494s,   51.32/s)  LR: 2.493e-03  Data: 0.001 (0.003)
[2025-08-23 20:01:01 ARES 2.0] INFO: Train: [0/1] [2450/10009 ( 24%)]  Loss: 6.745 (6.86)  Time: 2.484s,   51.52/s  (2.494s,   51.33/s)  LR: 2.543e-03  Data: 0.001 (0.003)
[2025-08-23 20:03:05 ARES 2.0] INFO: Train: [0/1] [2500/10009 ( 25%)]  Loss: 6.749 (6.86)  Time: 2.482s,   51.58/s  (2.494s,   51.33/s)  LR: 2.593e-03  Data: 0.001 (0.003)
[2025-08-23 20:05:09 ARES 2.0] INFO: Train: [0/1] [2550/10009 ( 25%)]  Loss: 6.799 (6.86)  Time: 2.482s,   51.57/s  (2.493s,   51.34/s)  LR: 2.643e-03  Data: 0.001 (0.003)
[2025-08-23 20:11:31 ARES 2.0] INFO: Runtime distributed=True, world_size=2, rank=0, local_rank=0, device_id=0
[2025-08-23 20:11:32 ARES 2.0] INFO: Creating model: resnet50
[2025-08-23 20:11:32 ARES 2.0] INFO: Model resnet50 created, param count:25557032
[2025-08-23 20:11:32 ARES 2.0] INFO: Using native Torch AMP. Training in mixed precision.
[2025-08-23 20:11:32 ARES 2.0] INFO: Using native Torch DistributedDataParallel.
[2025-08-23 20:11:56 ARES 2.0] INFO: Scheduled epochs: 1
[2025-08-23 20:11:56 ARES 2.0] INFO: Start training for 1 epochs
[2025-08-23 20:16:44 ARES 2.0] INFO: Runtime distributed=True, world_size=2, rank=0, local_rank=0, device_id=0
[2025-08-23 20:16:44 ARES 2.0] INFO: Creating model: resnet50
[2025-08-23 20:16:44 ARES 2.0] INFO: Model resnet50 created, param count:25557032
[2025-08-23 20:16:44 ARES 2.0] INFO: Using native Torch AMP. Training in mixed precision.
[2025-08-23 20:16:44 ARES 2.0] INFO: Using native Torch DistributedDataParallel.
[2025-08-23 20:16:49 ARES 2.0] INFO: Scheduled epochs: 1
[2025-08-23 20:16:49 ARES 2.0] INFO: Start training for 1 epochs
[2025-08-23 20:17:06 ARES 2.0] INFO: Train: [0/1] [   0/10009 (  0%)]  Loss: 6.923 (6.92)  Time: 17.782s,    7.20/s  (17.782s,    7.20/s)  LR: 1.000e-04  Data: 4.172 (4.172)
